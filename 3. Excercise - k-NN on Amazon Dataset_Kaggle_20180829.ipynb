{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## **3] Apply k-NN on Amazon reviews data-set [M]**\n1. Apply k-NN brute and kd-tree\n2. Featurize / Vectorize: using BoW, tf-idf, avg w2v, if-idf weighted w2v\n3. Splitting of data set into train and test: Apply time based slicing. 70% fata is train dataset and rest 30% becomes test dataset\n4. **Then report the accuracy:** 10 fold cross validation to find optimal k in k-NN. Report the test accuracy for all dataset vectors BoW, tf-idf, avg w2v, if-idf weighted w2v"
    },
    {
      "metadata": {
        "_uuid": "fcccdc8287eea556a3f10288fea170fe5e214c44"
      },
      "cell_type": "markdown",
      "source": "## **SOLUTION**:"
    },
    {
      "metadata": {
        "_uuid": "c97b0830aaefba2aa1495d8c166518a0fdfc0038"
      },
      "cell_type": "markdown",
      "source": "**I am using kaggle.com to complete exercise. I have pre-processed the data and stored the result is csv file and then commented the pre-processing code below.**"
    },
    {
      "metadata": {
        "_uuid": "5ae51fbf30967900d279a1fdcae515d15f2bf6ee"
      },
      "cell_type": "markdown",
      "source": "**I am using pre-processing code given by you in the course**"
    },
    {
      "metadata": {
        "_uuid": "966e3c71928f7f67841375c42fa075165883ed01"
      },
      "cell_type": "markdown",
      "source": "## TEXT PREPROCESSING"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n\n%matplotlib inline\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\n# using the SQLite Table to read data.\n#con = sqlite3.connect('./amazon-fine-food-reviews/database.sqlite') \n#con = sqlite3.connect('../input/database.sqlite') \ncon = sqlite3.connect('../input/database.sqlite') \n\n#filtering only positive and negative reviews i.e. \n# not taking into consideration those reviews with Score=3\nfiltered_data = pd.read_sql_query(\"\"\"\n ###SELECT *\n ###FROM Reviews\n ###WHERE Score != 3\n\"\"\", con) \n\n# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'\n\n#changing reviews with score less than 3 to be positive and vice-versa\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition) \nfiltered_data['Score'] = positiveNegative\n\ndisplay= pd.read_sql_query(\"\"\"\n ###SELECT *\n ###FROM Reviews\n ###WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n ###ORDER BY ProductID\n\"\"\", con)\ndisplay\n\n#Sorting data according to ProductId in ascending order\nsorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n\n#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape\n\n#Checking to see how much % of data still remains\n(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100\n\ndisplay= pd.read_sql_query(\"\"\"\n ###SELECT *\n ###FROM Reviews\n ###WHERE Score != 3 AND Id=44737 OR Id=64422\n ###ORDER BY ProductID\n\"\"\", con)\ndisplay\n\nfinal=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51db28a3aa0500a181acf05abaf3fa0aa199cef1"
      },
      "cell_type": "markdown",
      "source": "**Text Preprocessing: Stemming, stop-word removal and Lemmatization**"
    },
    {
      "metadata": {
        "_uuid": "3545229b1914eb438b88723c22caeb5f46c25ec5"
      },
      "cell_type": "markdown",
      "source": "I am using the code given by you for pre-processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbad5b56d108edd4c1e57bb37fcb723609258718",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n# find sentences containing HTML tags\nimport re\ni=0;\nfor sent in final['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;    \n\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english')) #set of stopwords\nsno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    return  cleaned\n\n#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n# this code takes a while to run as it needs to run on 500k sentences.\ni=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    #Comment above line to remove stemming an replce this line with below line\n                    #s=cleaned_words.lower()\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'positive': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)  ## Replace str1 by str1.lstrip('b\\'').rstrip('\\'')\n    i+=1\n    \nfinal['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \ncleanedText=final['CleanedText']\nfinalScore = final['Score']\nprint(finalScore.head(2))\nprint(cleanedText.head(2))\nprint(\"Shape cleanedText: \",cleanedText.shape)\nprint(\"Shape: finalScore\",finalScore.shape)\n\nfinal1=pd.DataFrame()\nfinal1['Text'] = final['Text']\nfinal1['CleanedText']  = final['CleanedText']\nfinal1['Time']  = final['Time']\nfinal1['Score'] = finalScore\n\nfinal1.to_csv('final1.csv')   \n\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c427b133e2f7dfb617ac27f09ae0a1fc9594bc9c"
      },
      "cell_type": "markdown",
      "source": "**Importing required libraries**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3239346b003cddaaf07eca2bc66cbe6d08293a41",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cross_validation import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import cross_validation",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "545b7db3eb539e1fe56139dbd968bb571ab5ea81"
      },
      "cell_type": "markdown",
      "source": "**Preprocessed data was stored in csv file. Reading the same into panda data frame variable data**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02daf1db13486115b637d87fcf27d1856495b9f0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#data.CleanedText[1]=data.CleanedText[1].lstrip('b\\'').rstrip('\\'')\n#data.CleanedText[1]\n#data.CleanedText[]=data.CleanedText[].lstrip('b\\'').rstrip('\\'')\n#(txt. for txt in data.CleanedText)\n#data.CleanedText[1]\ndata=pd.read_csv('../input/final1-with-time/final1 (1).csv') \n\"\"\"\ni=0\nfor sent in data.CleanedText:\n    data.CleanedText[i]=sent.lstrip('b\\'').rstrip('\\'')\n    i+=1\n#CleanedText_trimmed=data.CleanedText\nprint(CleanedText.head)\ndata.sort_values(['Time'], ascending=1)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "533ffdf894f00bd55de81a10bdccedd6f21b1710"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "fa8f0f33ba989205c220971cfbe0843f7a29c8ad"
      },
      "cell_type": "markdown",
      "source": "## Defining below functions"
    },
    {
      "metadata": {
        "_uuid": "ec5a3ea48ee2b25991f75f06301450873e8b904c"
      },
      "cell_type": "markdown",
      "source": "**1. Function knn10fold_and_MSE**\n* This function takes training data both X_train and Y_train to train the model\n* This function also takes argument algo where in we can provide whether to use brute or kd_tree\n* We need to do time based splitting but not random while training 10 fold cross validation kNN model; I am using TimeSeriesSplit and passing 10 as argument and my traing data is already sorted in ascending order.\n* I did added print statements to undestand the execution after writing code. Now those print statements are commented.\n*  This function plots the graph \"Number of Neighbors K')\" against \"Misclassification Error\" \n* It takes average accuracy of 10 folds for each k and then calculates the mis-classification error as (1- average_accuracy_for_each_k)\n* It then select the optimal k such that misclassification error is minimum and returns the same"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da0721999ee1e2ee73eb3bbe38f59e82601abdc6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def knn10fold_and_MSE(x_train, y_train, algo):\n    neighbors = list(range(1,20,2))\n    cv_scores = []\n    # perform 10-fold cross validation\n    time_series_10foldcv = TimeSeriesSplit(n_splits=10)\n    for k in neighbors:\n        #print(\"kNN for k=\",k)\n        #knn = KNeighborsClassifier(n_neighbors=k, weights='distance', algorithm=algo)\n        knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm=algo)\n        #Below function does random k' folds and not time based, that why this is not used\n        #scores = cross_val_score(knn, x_train, y_train, cv=10, scoring='accuracy')\n        this_cv_scores=[]\n        for train_index, cv_index in time_series_10foldcv.split(x_train):\n            # tscv_data.split(x_train) : This will give 10 iterations \n            # Each iteration has train and cross validation indexes based on time but not the random splitting\n            # But not the random creation of train and cross validation data sets\n            \n            ######### We get the indexes in sequence since TimeSeriesSplit i.t time based splitting ######### \n            ##print(\"Train set index min :\", np.min(train_index))\n            ##print(\"Train set index max :\", np.max(train_index))\n            ##print(\"CV set index min    :\", np.min(cv_index))\n            ##print(\"CV set index max    :\", np.max(cv_index))\n            \n            X_train, X_cv = x_train[train_index], x_train[cv_index]            \n            Y_train, Y_cv = y_train[train_index], y_train[cv_index]\n                                  \n            #Randomly choosing data points to train the model\n            #sample_indices = np.random.choice(train_index[-1],20000)\n            #x_train_sample = X_train[sample_indices]\n            #y_train_sample = Y_train.iloc[sample_indices]            \n            #knn.fit(x_train_sample, y_train_sample)\n            \n            knn.fit(X_train, Y_train)\n            pred = knn.predict(X_cv)\n            accuracy = accuracy_score(Y_cv, pred, normalize=True)\n            #print(\"Accuracy:\",accuracy)\n            this_cv_scores.append(accuracy)\n                        \n        ##print(\"k=\",k, \", 10 folds cross validation scores \", this_cv_scores)\n        ##print(\"k=\",k, \", Agerage is: \", np.average(this_cv_scores))\n        cv_scores.append(np.average(this_cv_scores))    \n    # changing to misclassification error\n    MSE = [1 - x for x in cv_scores]    \n    # determining best k\n    best_k_index = MSE.index(min(MSE))\n    optimal_k = neighbors[best_k_index]\n    CV_Accuracy = cv_scores[best_k_index] * 100\n    # plot misclassification error vs k \n    plt.plot(neighbors, MSE)    \n    for xy in zip(neighbors, np.round(MSE,3)):\n        plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')    \n    plt.xlabel('Number of Neighbors K')\n    plt.ylabel('Misclassification Error')\n    plt.show()\n    print(\"The misclassification error for each k value is : \", np.round(MSE,3)) \n    print('\\nThe optimal number of neighbors is %d and respective accuracy over training data or cross validation accuracy is %f%%' % (optimal_k,CV_Accuracy)) \n       \n    return optimal_k\n##########################################################################################",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5cbd6f8ee4dd75ba0bd808f2172c7ea0f5af789f"
      },
      "cell_type": "markdown",
      "source": "**2. Function knn_with_optimal_k**\n* Here, lets test the model over unknown data which is x_test and y_test here\n* Train the model using x_train and y_train; then predict using x_test\n* Then get the accuracy using prected class labels and y_train and report the accuracy for optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a222b9bbfb77f9e099b305e1e14694b2ac49dd1d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def knn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, algo):\n    # ============================== KNN with k = optimal_k ===============================================\n    # instantiate learning model k = optimal_k\n    #knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k, weights='distance', algorithm=algo)\n    knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k, weights='uniform', algorithm=algo)\n    # fitting the model\n    knn_optimal.fit(x_train, y_train)\n    # predict the response\n    pred = knn_optimal.predict(x_test)\n    # evaluate accuracy\n    acc = accuracy_score(y_test, pred) * 100\n    print('\\nThe test accuracy or accuracy over unknown data of the ', algo ,' knn classifier for optimal k = %d is %f%%' % (optimal_k, acc))\n    print('\\n')\n    print('\\n#############################################################################################') \n    print('\\nOBSERVATION: Able to achieve ', acc, '% of test accuracy using ', algo ,' knn classifier' )\n    print('\\n#############################################################################################')\n    print('\\n')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f734118634fa86b7deeaa5354b9fc2a9a034d575"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "9d5987d4260436e1dd0815dd0d0a0a558b6b9063"
      },
      "cell_type": "markdown",
      "source": "## Sorting data in ascending order since time based splitting"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c56eefd2702c7420a6a60924773b674b6c9cce0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Sorting the data on ascending order of Time since time based splitting needs to be done further\ndata=data.sort_values(['Time'], ascending=1)\nprint(\"Time min:\", data['Time'].min())\nprint(\"Time max:\", data['Time'].max())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39b62bc73ee8f82a980aeb9213ddf9dfdfaeca5b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#import numpy as np\n#from sklearn.cross_validation import train_test_split\n#a = np.arange(10).reshape((5, 2))\n#b = np.arange(5)\n#print(\"Array a: \\n\",a); print(\"Array b: \\n\",b);\n\n#a_train, a_test, b_train, b_test = train_test_split(a, b,  test_size=0.3,  random_state=0)\n#print(\"a_train :\\n\", a_train)\n#print(\"b_train :\\n\", b_train)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "14b453c2eb4d4bba2e77d59f8afcd7cb93ccfd44"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "772b9a6b310f658bdc2e9fd1afd2e0591927cddd"
      },
      "cell_type": "markdown",
      "source": "## **BAG OF WORDS**"
    },
    {
      "metadata": {
        "_uuid": "8ba9b52b17d2a383d68fd6b27a421f3f9bf55117"
      },
      "cell_type": "markdown",
      "source": "**Getting the BoW vector representation for all reviews**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8c72de9ab0e0a20752ce2a40985dbc67a777214",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#BoW\ncount_vect = CountVectorizer() #in scikit-learn\nfinal_counts_bow = count_vect.fit_transform(data.CleanedText.values)\nfinal_counts_bow.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b086043f2441e269604e82022f103f866216182"
      },
      "cell_type": "markdown",
      "source": "**Processing only 5000 records since entire dataset processing takes too much RAM**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4b6247f2907043311d37bb94afb977ac68391c5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(type(final_counts_bow))\n#X = final_counts_bow[0:50000,:].toarray() \n#Y = np.array(data['Score'][0:50000])\n### Mention the total number of data points to be worked on since I have machine with low configuration\nnum_of_points=5000\nX = final_counts_bow[0:num_of_points,:].toarray() \nY = np.array(data['Score'][0:num_of_points])\nprint(\"X Shape : \",X.shape, \" X Ndim: \",X.ndim)\nprint(\"Y Shape : \",Y.shape, \" Y Ndim: \",Y.ndim)\n#print(X); print(Y)\ndata['Score'][0:num_of_points].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b5f88efd710f204c5cbd770ffa69001416c813d"
      },
      "cell_type": "markdown",
      "source": "**Splitting dataset into train and test with 70:30 ratio**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c6dc5b43a01b5d69986f436d2df29bd918eb3b3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train to test ratio is 70:30\nboundry=int(num_of_points*0.7)\nprint(\"Boundry: \", boundry)\n# split the data set into train and test based in time and not random splitting\nx_train = X[:boundry]; x_test = X[boundry:]\ny_train = Y[:boundry]; y_test = Y[boundry:]\n\n#x_train, x_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=0.3, random_state=0)\nprint(\"X train Shape : \",x_train.shape, \" X Ndim: \",x_train.ndim); print(\"Y train Shape : \",y_train.shape, \" Y Ndim: \",y_train.ndim)\nprint(\"X test Shape : \",x_test.shape, \" X Ndim: \",x_test.ndim); print(\"Y test Shape : \",y_test.shape, \" Y Ndim: \",y_test.ndim)\n#print(\"Time min:\", data['Time'].min())\n#print(\"Time max:\", data['Time'].max())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b6876c04989a86acf00147aeac3c626a2e3db28a"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  Brute 10 fold k-NN for BoW**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN Brute algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12c00bdb20ee5d1662478a215a29937a440cf9e2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'brute')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'brute')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "54b5f68b823ccb5e0c09f0269f5e0fcb018d78ea"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  kd-tree 10 fold k-NN for BoW**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN kd_tree algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71283836844e4a2e1e1bacfb840e0b76375047ed",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'kd_tree')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'kd_tree')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f4d641ab141b0cd3db5dcb2816a783e15c0a405e"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "e75a9da22f1672565ce327c37d6c746471f23880"
      },
      "cell_type": "markdown",
      "source": "## TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY "
    },
    {
      "metadata": {
        "_uuid": "98cf7b00ad8f51234f55e8a29cddf1f180f45440"
      },
      "cell_type": "markdown",
      "source": "**Getting the tf-idf vector representation for all reviews**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05d84358793906774f64f7c7bea1c48399fb5d8f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#TF-IDF\ntf_idf_vect = TfidfVectorizer()\nfinal_tf_idf = tf_idf_vect.fit_transform(data.CleanedText.values)\nprint(\"final_tf_idf shape: \",final_tf_idf.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ba2fa0fdc47c96a55bceb78f312ca3e5b009dc6"
      },
      "cell_type": "markdown",
      "source": "**Processing only 5000 records since entire dataset processing takes too much RAM**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da14375036feaf11f76cc6282ec3bf6873b9bb05",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Define data points and labels\n### Mention the total number of data points to be worked on since I have machine with low configuration\nnum_of_points=5000\nX = final_tf_idf[0:num_of_points,:].toarray() \nY = np.array(data['Score'][0:num_of_points])\nprint(\"X Shape : \",X.shape, \" X Ndim: \",X.ndim)\nprint(\"Y Shape : \",Y.shape, \" Y Ndim: \",Y.ndim)\ndata['Score'][0:num_of_points].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d3dceb601369a58f48d50de38b8d4ad026bf6997"
      },
      "cell_type": "markdown",
      "source": "**Splitting dataset into train and test with 70:30 ratio**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "107c03c605c9ef3a34e9eb34b2e0defe7023242e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train to test ratio is 70:30\nboundry=int(num_of_points*0.7)\nprint(\"Boundry: \", boundry)\n# split the data set into train and test based in time and not random splitting\nx_train = X[:boundry]; x_test = X[boundry:]\ny_train = Y[:boundry]; y_test = Y[boundry:]\n\n#x_train, x_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=0.3, random_state=0)\nprint(\"X train Shape : \",x_train.shape, \" X Ndim: \",x_train.ndim); print(\"Y train Shape : \",y_train.shape, \" Y Ndim: \",y_train.ndim)\nprint(\"X test Shape : \",x_test.shape, \" X Ndim: \",x_test.ndim);    print(\"Y test Shape : \",y_test.shape, \" Y Ndim: \",y_test.ndim)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e82f4b82a306170545d8166fb92a5dbdc6599dad"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  Brute 10 fold k-NN for TF-IDF**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN Brute algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99bc4607cc22c9ba4deef4497a36b64625c7418e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'brute')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'brute')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8cb1af5a1943d7bd09df75dc1374c095009ae6cb"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  kd-tree 10 fold k-NN for TF-IDF**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN kd_tree algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e71eedc4313c699bf45ff57a79884e9adcd325c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'kd_tree')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'kd_tree')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96465d19a879f432d3c46c65c84b4b8e2669c826"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "cebef7ae4d8c1ec0d41355600baa5410ceb94e5a"
      },
      "cell_type": "markdown",
      "source": "**I'm using google word to vector to measure semantic similarities**"
    },
    {
      "metadata": {
        "_uuid": "f075c98c50e4eff836ccd930c9505d83340da78f"
      },
      "cell_type": "markdown",
      "source": "## **Word2Vec**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fefc1d53a81cda25391f28c40312fad9b7a96a17",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\nmodel = KeyedVectors.load_word2vec_format('../input/word2vec-model/GoogleNews-vectors-negative300.bin', binary=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e1b8e14a3135a7f7b9d60b3a7d815944d491f82d"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "595a471cb82f1dac573c33abf43c784887481159"
      },
      "cell_type": "markdown",
      "source": "## **Average Word2Vec**"
    },
    {
      "metadata": {
        "_uuid": "307863b1a6f6be86708ad57bf75c49470b41e52a"
      },
      "cell_type": "markdown",
      "source": "**Processing only 5000 records since entire dataset processing takes too much RAM**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ed70f86c92f8315a6aea043c9d3811a26196549",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Mention the total number of data points to be worked on since I have machine with low configuration\nnum_of_points=5000\n# Define data points and labels\nX = data.CleanedText[0:num_of_points]\n#Y = np.array(data['Score'][0:500])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ace6abf596f5c69d74af363b68a50c40864b307f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d16298aa669ef728a54289538479ffb50758cf4"
      },
      "cell_type": "markdown",
      "source": "**Generating the Average word3vec vector representation for all reviews. Getting vector for each word in review and creating new scentense vector such that its an average of word2vector of all the words in review text**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ebf9378fee4b75c2208f85530bfa4845f771737",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# average Word2Vec\n# compute average word2vec for each review.\ni=0\nsent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\nfor sent in X: # for each review/sentence\n    #if (i == 1626 or i == 1698 or i == 1950 or i == 4532 or i == 4752 ):\n        #print(\"i=\",i,\" sent: \", sent)\n    #print(\"Sentense : \", sent, \"Type: \",type(sent))\n    sent_vec = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    #for word in str(sent).split():\n        #print(\"word: \", str(word))\n    for word in str(sent).split(): # for each word in a review/sentence\n        try:\n            #print(\"word : \", word)\n            vec = model.wv[word]\n            #print(\"vec : \", vec.shape)\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            #print(\"Exception\")\n            pass\n    \"\"\"\n    if (i == 1626 or i == 1698 or i == 1950 or i == 4532 or i == 4752 ):\n        print(\"sent: \", sent)\n        print(\"cnt_words=\",cnt_words)\n        print(\"i=\",i,\" sent_vec: \", sent_vec)\n    \"\"\"\n    if (cnt_words !=0):\n        sent_vec /= cnt_words\n        ## If we divide zero by zero then it generated NaN will cause issue further\n    \"\"\"            \n    if (i == 1626 or i == 1698 or i == 1950 or i == 4532 or i == 4752 ):\n        print(\"i=\",i,\" sent_vec: \", sent_vec)\n    \"\"\"\n    sent_vectors.append(sent_vec)\n    i+=1\nprint(len(sent_vectors))\nprint(len(sent_vectors[0]))\n#print(\"Shape: \",sent_vectors.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8378e867d3446218fe588be4c1b4358993ac8119"
      },
      "cell_type": "markdown",
      "source": "########################################################"
    },
    {
      "metadata": {
        "_uuid": "257fd0f0f40dccad97d712546941abac80865b85"
      },
      "cell_type": "markdown",
      "source": "## I was facing below error and fixed the same"
    },
    {
      "metadata": {
        "_uuid": "14611a62c66f7cb7fdf34898d92abd2c8fa4b1bf"
      },
      "cell_type": "markdown",
      "source": "** ValueError: Input contains NaN, infinity or a value too large for dtype('float64').**\n* ---------------------------------------------------------------------------\n* ValueError                                Traceback (most recent call last)\n* <ipython-input-57-2727e6eef3b2> in <module>()\n* ----> 1 optimal_k = knn10fold_and_MSE(x_train, y_train, 'brute')\n* ---> 34             pred = knn.predict(X_cv)\n*  --> 143         X = check_array(X, accept_sparse='csr')\n*  --> 453             _assert_all_finite(array)\n* ---> 44                          \" or a value too large for %r.\" % X.dtype)\n*  ValueError: Input contains NaN, infinity or a value too large for dtype('float64')."
    },
    {
      "metadata": {
        "_uuid": "4cd58448da6c53e525a927eae9deef606820879a"
      },
      "cell_type": "markdown",
      "source": "**Root cause of the issue: Below words in below pre-processed review texts (which are as sent in below cell for 5 of the review texts) does not have vector in googles word2vec. We can train own word to vector as well**"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "52090b6ae3d756ab5904428f3e67fd9f6007225c"
      },
      "cell_type": "code",
      "source": "\n#for i in range(0,5000):\n#    for j in range(0,300):\n#        if(np.isnan(X[i][j])):\n#            print(\"i=\",i,\" j=\",j)\n#            break\n            \n    # i= 1626  j= 0\n    # i= 1698  j= 0\n    # i= 1950  j= 0\n    # i= 4532  j= 0\n    # i= 4752  j= 0\n\n\n#[4752]\n#ent_vectors[4752]\n#data.CleanedText[1626]\n\"\"\"\ni= 1626  sent:  b'worth'\ni= 1698  sent:  b'excel'\ni= 1950  sent:  b'calori noodl delici fill'\ni= 4532  sent:  b'good chewi salti sweet'\ni= 4752  sent:  b'tea delici hestit buy'\n\"\"\"\n#model.wv['hestit']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6544b2cee84d52680d526969d733d5f87efd8043"
      },
      "cell_type": "code",
      "source": "########################################################",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b1fbce9ab9869c9402d7371149f5598faff250e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X = np.array(sent_vectors)\nY = np.array(data['Score'][0:num_of_points])\nprint(\"X Shape : \",X.shape, \" X Ndim: \",X.ndim)\nprint(\"Y Shape : \",Y.shape, \" Y Ndim: \",Y.ndim)\ndata['Score'][0:num_of_points].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fcc43584f9c12732fecfe2fef4815db8ee5da320"
      },
      "cell_type": "markdown",
      "source": "**Splitting dataset into train and test with 70:30 ratio**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1a357a973601aac43a5cfe76f385bdf72387a44",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n# Train to test ratio is 70:30\nboundry=int(num_of_points*0.7)\nprint(\"Boundry: \", boundry)\n# split the data set into train and test based in time and not random splitting\nx_train = X[:boundry]; x_test = X[boundry:]\ny_train = Y[:boundry]; y_test = Y[boundry:]\n\n#x_train, x_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=0.3, random_state=0)\nprint(\"X train Shape : \",x_train.shape, \" X Ndim: \",x_train.ndim); print(\"Y train Shape : \",y_train.shape, \" Y Ndim: \",y_train.ndim)\nprint(\"X test Shape : \",x_test.shape, \" X Ndim: \",x_test.ndim);    print(\"Y test Shape : \",y_test.shape, \" Y Ndim: \",y_test.ndim)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "559eae73f1b38c00dfdc859012be675c8040a50b"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  Brute 10 fold k-NN for Avg word2vec**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN Brute algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4867fe42489243057da020b03ff7de978231683c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'brute')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'brute')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c155a6d757e22dafb558374cf982bd58ca5a406e"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  kd_tree 10 fold k-NN for Avg word2vec**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN kd_tree algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e355a9fb0700b6c02ba3a9ada31d28d17b921ace",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'kd_tree')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'kd_tree')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8826fabafbb08024ac854927dbf51eeb8153094d"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "b50fffac9f21135a28672f4a2ac79b448da92fda"
      },
      "cell_type": "markdown",
      "source": "## **TF-IDF weighted Word2Vec**"
    },
    {
      "metadata": {
        "_uuid": "d05c7051ec53468ce04105abad5e2e31d7b40813"
      },
      "cell_type": "markdown",
      "source": "**Processing only 5000 records since entire dataset processing takes too much RAM**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e97177169730758e6abab6ebfbc53d26d0fef856",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Mention the total number of data points to be worked on since I have machine with low configuration\nnum_of_points=5000\nX = data.CleanedText[0:num_of_points]\n# TF-IDF weighted Word2Vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5cb940e19119f441d29118d0fdb603cb79e10871"
      },
      "cell_type": "markdown",
      "source": "**Getting vector for each word in review and creating new scentense vector such that word2vector is been weighted with respective TF-IDF of the review**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fa5bcc0ad7702c9b1f8803e8d31d3a557a155c2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\ntfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\nrow=0;\nfor sent in X: # for each review/sentence\n    sent_vec = np.zeros(300) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence/review\n    #sent='littl book make son laugh loud recit car drive along alway sing refrain hes learn whale india droop love new word book introduc silli classic book will bet son still abl recit memori colleg'\n    #print(\"Sentense : \", sent)\n    for word in sent.split(): # for each word in a review/sentence\n        try:\n            #print(\"Word: \", word)\n            vec = model.wv[word]\n            #print(\"Vector: \", vec, \" Size: \", vec.shape)\n            # obtain the tf_idfidf of a word in a sentence/review\n            tf_idf = final_tf_idf[row, tfidf_feat.index(word)]\n            #print(\"tfidf: \", tf_idf)\n            sent_vec += (vec * tf_idf)\n            #print(\"sent_vec Calculated\")\n            weight_sum += tf_idf\n        except:\n           # print(\"Exception\")\n            #e = sys.exc_info()[0]\n            #print(\"Exception: \",e)\n            ##write_to_page( \"<p>Error: %s</p>\" % e )\n            pass\n    zero_weight_sum_count=0\n    if(weight_sum != 0):\n        sent_vec /= weight_sum\n    else        :\n        zero_weight_sum_count += 1\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\n    \nprint(len(tfidf_sent_vectors))\nprint(len(tfidf_sent_vectors[0])) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3ce19d2f62a251961840cfaa2e782f3b4d89b8c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X = np.array(tfidf_sent_vectors)\nY = np.array(data['Score'][0:num_of_points])\nprint(\"X Shape : \",X.shape, \" X Ndim: \",X.ndim)\nprint(\"Y Shape : \",Y.shape, \" Y Ndim: \",Y.ndim)\ndata['Score'][0:num_of_points].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d3f4cc0a120ae5c37313ee8dcd8e05ff54a2c20b"
      },
      "cell_type": "markdown",
      "source": "**Splitting dataset into train and test with 70:30 ratio**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61208725d01b4beef4e9c5ff29f8faa6e5b90ae1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train to test ratio is 70:30\nboundry=int(num_of_points*0.7)\nprint(\"Boundry: \", boundry)\n# split the data set into train and test based in time and not random splitting\nx_train = X[:boundry]; x_test = X[boundry:]\ny_train = Y[:boundry]; y_test = Y[boundry:]\n\n#x_train, x_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=0.3, random_state=0)\nprint(\"X train Shape : \",x_train.shape, \" X Ndim: \",x_train.ndim); print(\"Y train Shape : \",y_train.shape, \" Y Ndim: \",y_train.ndim)\nprint(\"X test Shape : \",x_test.shape, \" X Ndim: \",x_test.ndim);    print(\"Y test Shape : \",y_test.shape, \" Y Ndim: \",y_test.ndim)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e0657e082caa061fedf1d6f06e660e2013d06d47"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  Brute 10 fold k-NN for TF-IDF word2vec**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN Brute algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be4a05afb4857d846f51337ba1b73776c26aedd4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'brute')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'brute')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de3f8dfa59141af10727a8404fdeb7c9e749c516"
      },
      "cell_type": "markdown",
      "source": "**Optimal k accuracy:  kd_tree 10 fold k-NN for TF-IDF word2vec**\n* Performing kNN 10 fold cross validation over training data to get the accuracy and mis classification error using kNN kd_tree algorithm\n* Getting optimal k and with low misclassification error\n* Then accuracy for test data is reported using optimal k"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d28069e4a2554c7e6dd9a2b722a5b670e429191e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optimal_k = knn10fold_and_MSE(x_train, y_train, 'kd_tree')\nknn_with_optimal_k(x_train, y_train, x_test, y_test, optimal_k, 'kd_tree')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "971312274cf7f36470457da4801790d7e01147e8"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3adbdfabf6fd6925b8b47a6f110bfc0345b56bcb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd065dcab8e50b23a7e15b894725153361ec4a4a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}